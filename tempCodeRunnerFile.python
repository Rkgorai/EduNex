# import pandas as pd
# from sklearn.model_selection import cross_val_predict, KFold, GridSearchCV
# from sklearn.metrics import mean_absolute_error
# from sklearn.preprocessing import RobustScaler, PolynomialFeatures
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
# from sklearn.linear_model import ElasticNet, Ridge, Lasso
# import lightgbm as lgb
# import xgboost as xgb
# from sklearn.feature_selection import RFECV

# # Load training and test data
# train_data = pd.read_csv('/Users/saarimkhan/Downloads/wild-blueberryyield-regression/train.csv')
# test_data = pd.read_csv('/Users/saarimkhan/Downloads/wild-blueberryyield-regression/test.csv')

# # Selected features
# selected_features = [
#     'RainingDays', 'osmia', 'AverageOfUpperTRange', 
#     'fruitset', 'fruitmass', 'seeds'
# ]

# # Separate features and target in the training data
# X = train_data[selected_features]
# y = train_data['yield']

# # Scale the features with RobustScaler to minimize the influence of outliers
# scaler = RobustScaler()
# X_scaled = scaler.fit_transform(X)

# # Initialize base models
# base_models = {
#     'LightGBM': lgb.LGBMRegressor(random_state=42),
#     'RandomForest': RandomForestRegressor(random_state=42),
#     'GradientBoosting': GradientBoostingRegressor(random_state=42),
#     'XGBoost': xgb.XGBRegressor(random_state=42)
# }

# # Expanded hyperparameter grids
# param_grids = {
#     'LightGBM': {
#         'n_estimators': [500, 800, 1000],
#         'learning_rate': [0.005, 0.01, 0.05],
#         'max_depth': [5, 7, 10, 15],
#         'num_leaves': [20, 31, 50, 100],
#         'min_child_samples': [5, 10, 20],
#         'colsample_bytree': [0.7, 0.9, 1.0]
#     },
#     'RandomForest': {
#         'n_estimators': [300, 500, 800],
#         'max_depth': [None, 10, 20],
#         'min_samples_split': [2, 5, 10],
#         'min_samples_leaf': [1, 2, 4],
#         'max_features': ['auto', 'sqrt', 'log2']
#     },
#     'GradientBoosting': {
#         'n_estimators': [300, 500, 800],
#         'learning_rate': [0.01, 0.05, 0.1],
#         'max_depth': [3, 5, 7],
#         'subsample': [0.8, 1.0]
#     },
#     'XGBoost': {
#         'n_estimators': [300, 500, 800],
#         'learning_rate': [0.01, 0.05, 0.1],
#         'max_depth': [5, 7, 10],
#         'subsample': [0.7, 0.8, 1.0],
#         'colsample_bytree': [0.7, 0.8, 1.0]
#     }
# }

# # Set up cross-validation
# kf = KFold(n_splits=10, shuffle=True, random_state=42)

# # Cross-validation predictions for base models with grid search
# cv_predictions = {}
# best_base_models = {}
# for name, model in base_models.items():
#     # Hyperparameter tuning with expanded grid
#     grid_search = GridSearchCV(model, param_grid=param_grids[name],
#                                cv=3, scoring='neg_mean_absolute_error',
#                                n_jobs=-1, verbose=1)
#     grid_search.fit(X_scaled, y)
#     best_base_models[name] = grid_search.best_estimator_
#     print(f"Best {name} parameters: {grid_search.best_params_}")

#     # Out-of-fold predictions for stacking
#     y_pred_cv = cross_val_predict(best_base_models[name], X_scaled, y, cv=kf)
#     cv_predictions[name] = y_pred_cv
#     mae = mean_absolute_error(y, y_pred_cv)
#     print(f"MAE for {name} with cross-validation: {mae}")

# # Prepare the stacked features (out-of-fold predictions of base models)
# X_stacked = pd.DataFrame(cv_predictions)

# # Recursive Feature Elimination with Cross-Validation (RFECV)
# final_model = ElasticNet(max_iter=10000)
# rfecv = RFECV(estimator=final_model, step=1, cv=kf, scoring='neg_mean_absolute_error')
# rfecv.fit(X_scaled, y)
# print("Optimal number of features selected by RFECV:", rfecv.n_features_)

# # Use reduced features based on RFECV
# X_reduced = X_scaled[:, rfecv.support_]
# X_test_reduced = scaler.transform(test_data[selected_features])[:, rfecv.support_]

# # Final Stacking Model using ElasticNet (can be swapped with Ridge or Lasso)
# final_stacking_model = StackingRegressor(
#     estimators=[(name.lower(), best_base_models[name]) for name in best_base_models],
#     final_estimator=ElasticNet(max_iter=10000),
#     cv=kf
# )
# final_stacking_model.fit(X_reduced, y)

# # Predict yield for the test set using reduced features
# test_preds_stacked = final_stacking_model.predict(X_test_reduced)

# # Create a DataFrame for the submission
# submission_stacked = pd.DataFrame({
#     'id': test_data['id'],
#     'yield': test_preds_stacked
# })
# # Save the submission to a CSV file
# submission_stacked.to_csv('/Users/saarimkhan/Downloads/z3_1.csv', index=False)
# print("Submission file for the stacked model saved successfully.")
import pandas as pd
from sklearn.model_selection import cross_val_predict, KFold, GridSearchCV
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import RobustScaler, PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import ElasticNet, Ridge, Lasso
import lightgbm as lgb
import xgboost as xgb
from sklearn.feature_selection import RFECV
# Load training and test data
train_data = pd.read_csv('/Users/saarimkhan/Downloads/wild-blueberryyield-regression/train.csv')
test_data = pd.read_csv('/Users/saarimkhan/Downloads/wild-blueberryyield-regression/test.csv')

# Selected features
selected_features = [
    'RainingDays', 'osmia', 'AverageOfUpperTRange', 
    'fruitset', 'fruitmass', 'seeds'
]
# Separate features and target in the training data
X = train_data[selected_features]
y = train_data['yield']
# Scale the features with RobustScaler to minimize the influence of outliers
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
# Optional: Add polynomial features to capture nonlinear interactions
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X_scaled)
# Initialize base models with expanded parameter grids
base_models = {
    'LightGBM': lgb.LGBMRegressor(random_state=42),
    'RandomForest': RandomForestRegressor(random_state=42),
    'GradientBoosting': GradientBoostingRegressor(random_state=42),
    'XGBoost': xgb.XGBRegressor(random_state=42)
}
param_grids = {
    'LightGBM': {
        'n_estimators': [500, 800, 1200],
        'learning_rate': [0.005, 0.01, 0.03, 0.05],
        'max_depth': [5, 7, 10, 15],
        'num_leaves': [20, 31, 50, 100],
        'min_child_samples': [5, 10, 20],
        'colsample_bytree': [0.7, 0.9, 1.0]
    },
    'RandomForest': {
        'n_estimators': [300, 500, 800],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['auto', 'sqrt', 'log2']
    },
    'GradientBoosting': {
        'n_estimators': [300, 500, 800],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7],
        'subsample': [0.8, 1.0]
    },
    'XGBoost': {
        'n_estimators': [300, 500, 800],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [5, 7, 10],
        'subsample': [0.7, 0.8, 1.0],
        'colsample_bytree': [0.7, 0.8, 1.0]
    }
}
# Set up cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
# Cross-validation predictions for base models with grid search
cv_predictions = {}
best_base_models = {}
for name, model in base_models.items():
    # Hyperparameter tuning with expanded grid
    grid_search = GridSearchCV(model, param_grid=param_grids[name],
                               cv=5, scoring='neg_mean_absolute_error',
                               n_jobs=-1, verbose=1)
    grid_search.fit(X_poly, y)
    best_base_models[name] = grid_search.best_estimator_
    print(f"Best {name} parameters: {grid_search.best_params_}")
    # Out-of-fold predictions for stacking
    y_pred_cv = cross_val_predict(best_base_models[name], X_poly, y, cv=kf)
    cv_predictions[name] = y_pred_cv
    mae = mean_absolute_error(y, y_pred_cv)
    print(f"MAE for {name} with cross-validation: {mae}")
# Prepare the stacked features (out-of-fold predictions of base models)
X_stacked = pd.DataFrame(cv_predictions)
# Recursive Feature Elimination with Cross-Validation (RFECV)
final_model = ElasticNet(max_iter=10000)
rfecv = RFECV(estimator=final_model, step=1, cv=kf, scoring='neg_mean_absolute_error')
rfecv.fit(X_stacked, y)
print("Optimal number of features selected by RFECV:", rfecv.n_features_)
# Use reduced features based on RFECV
X_reduced = X_stacked.iloc[:, rfecv.support_]
X_test_poly = poly.transform(scaler.transform(test_data[selected_features]))
X_test_reduced = X_test_poly[:, rfecv.support_]
# Final Stacking Model using ElasticNet (can be swapped with Ridge or Lasso)
final_stacking_model = StackingRegressor(
    estimators=[(name.lower(), best_base_models[name]) for name in best_base_models],
    final_estimator=ElasticNet(max_iter=10000),
    cv=kf
)
final_stacking_model.fit(X_reduced, y)
# Predict yield for the test set using reduced features
test_preds_stacked = final_stacking_model.predict(X_test_reduced)
# Create a DataFrame for the submission
submission_stacked = pd.DataFrame({
    'id': test_data['id'],
    'yield': test_preds_stacked
})
# Save the submission to a CSV file
submission_stacked.to_csv('/Users/saarimkhan/Downloads/final_stacked_submission.csv', index=False)
print("Submission file for the fine-tuned stacked model saved successfully.")

